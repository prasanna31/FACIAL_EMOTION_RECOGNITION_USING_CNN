{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "435/449 [============================>.] - ETA: 47s - loss: 1.6756 - accuracy: 0.3295"
     ]
    }
   ],
   "source": [
    "#FACIAL EMOTION RECOGNITION\n",
    "#importing the required modules\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "#reading the dataset.This is the dataset which is available in internet.This is a csv file with columns \"emotion,pixels,usage\"\n",
    "dataset=pd.read_csv('./fer2013.csv')\n",
    "#array variables for storing training data for x axis and y axis ,testing data for x axis and y axis \n",
    "xtrain,ytrain,xtest,ytest=[],[],[],[]\n",
    "#seperating training data,private testing data and public testing data\n",
    "#looping through out the csv file and assigning values to particular arrays\n",
    "for index, row in dataset.iterrows():\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "           xtrain.append(np.array(val,'float32'))\n",
    "           ytrain.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "           xtest.append(np.array(val,'float32'))\n",
    "           ytest.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n",
    "\n",
    "#variables for number of features,number of labels,batch size i.e,how many parts the data set is divided.\n",
    "#number of features\n",
    "no_features = 64\n",
    "#number of labels i.e,no of emotions=happy,sad,angry,fear,surprized,disgust,neutral.\n",
    "no_lbls = 7\n",
    "#batch size=into how many parts the data is divided.\n",
    "bth_size = 64\n",
    "#epochs =the repetitive number of times the data is trainined by the model.\n",
    "no_epochs = 2\n",
    "#width and height of the image i.e,image is vertically divided into 48 pixels and horizontally divided into 48 pixels\n",
    "width, height = 48, 48\n",
    "\n",
    "#coverting normal arrays to one hot vectors so that the convolutional neural networks accept them.\n",
    "xtrain = np.array(xtrain,'float32')\n",
    "ytrain = np.array(ytrain,'float32')\n",
    "xtest = np.array(xtest,'float32')\n",
    "ytest = np.array(ytest,'float32')\n",
    "\n",
    "ytrain=np_utils.to_categorical(ytrain, num_classes=no_lbls)\n",
    "ytest=np_utils.to_categorical(ytest, num_classes=no_lbls)\n",
    "\n",
    "#normalizing the training data so that the training data is placed between 'zero' and 'one'\n",
    "xtrain -= np.mean(xtrain, axis=0)\n",
    "xtrain /= np.std(xtrain, axis=0)\n",
    "#normalizing the testing data so that the testing data lies between zero and one\n",
    "xtest -= np.mean(xtest, axis=0)\n",
    "xtest /= np.std(xtest, axis=0)\n",
    "#reshaping the data so that it is given as 2D input to the convolutional layers.\n",
    "xtrain = xtrain.reshape(xtrain.shape[0], 48, 48, 1)\n",
    "\n",
    "xtest = xtest.reshape(xtest.shape[0], 48, 48, 1)\n",
    "#designing a 'CONVOLUTIONAL NEURAL NETWORK' for training the model.\n",
    "model = Sequential()\n",
    "#adding 2 convolutional layers simultaneously and followed by a max pooling layer.\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',input_shape=(xtrain.shape[1:])))\n",
    "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "#dropout is added to the model so that the model we are training doesnot undergo overfitting.\n",
    "model.add(Dropout(0.5))\n",
    "#adding the next 2 convolutional layers and a maxpooling layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#adding the last 2 convolutional layers and a maxpooling layer.\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "#flattening the 2D convolutional layers into 1D fully connected layers.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#using the softmax so that the output of each label is produced as percentange.\n",
    "model.add(Dense(no_lbls, activation='softmax'))\n",
    "#creating a check point so that the best accurated training is taken into count whereas the least accurated one is ignored\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('fer.h5',\n",
    "                            monitor='loss',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='auto')\n",
    "\n",
    "#model is compiled and the accuracy metric is viewed.\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting the dataset to the model.i.e, training the model.\n",
    "model.fit(xtrain, ytrain,\n",
    "          batch_size=bth_size,\n",
    "          epochs=no_epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(xtest, ytest),\n",
    "          shuffle=True)\n",
    "\n",
    "\n",
    "#saving the model as a json file so that we can reuse the model.\n",
    "fer_json = model.to_json()\n",
    "with open(\"fer.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save_weights(\"fer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
